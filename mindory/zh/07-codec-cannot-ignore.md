# 构建阅读系统不可忽视的“小”问题

## "小问题" 引发大事故

在构建阅读系统时, 无论是电子书阅读器, 还是搜索引擎 —— 最容易被低估的往往不是算法或模型，而是两个看似 “微不足道” 的核心问题：

1. **字符编码（ GBK / GB18030 / UTF-8 ）**
2. **文本格式（ TXT / EPUB / MOBI / PDF ）**

这些问题处于系统的最底层，却如同地基一般：处理不当会导致数据无声损坏，甚至让系统崩溃。本文将分享一套经过生产环境验证的**实战解决方案**。

---

## 一、字符编码：世界不只有 UTF-8
### 1.1 现实痛点：编码混乱无处不在
在真实的文档处理场景中，编码问题远比想象中复杂：
- 大量中文文档采用传统编码：
  - GBK
  - GB2312
  - GB18030（兼容所有中文编码的国家标准）
- 海量文件存在两种情况：
  - **未声明编码格式**
  - **编码声明错误**
- 部分文档甚至在不同章节混用编码

任何**默认所有输入都是有效 UTF-8** 的系统，最终只会面临两种结局：
- 程序崩溃 
- 数据无声损坏（比崩溃更致命）

---

### 1.2 编码处理的三大核心原则
#### 原则 1: 在系统边界拦截原始字节
> **无效编码必须在最接近 I/O 的层处理**

实际落地逻辑：
- 上层模块（ 解析器 / 导入器 / 文本分割 / embedding ）：
  - **只处理有效 UTF-8 文本**
- 下层模块（ Importer / Codec ）：
  - 吸收所有混乱、不安全的原始输入

这种分层不是可选的 —— 而是系统的**稳定性保障**。

---

#### 原则 2：编码探测需遵循严格顺序
编码探测不能“盲目尝试直到成功”，必须遵循**固定优先级顺序**。

经过验证的核心策略：
```
严格验证 → 半自描述编码 → 宽松兼容 → 兜底方案
```

实战解码顺序：
1. **UTF-8（ 严格验证 ）**
2. **GBK（ 无效则快速失败 ）**
3. **GB18030（ 国家标准超集，覆盖最广 ）**
4. **UTF-8 有损解码（ 最终兜底 ）**

为何这一顺序有效：
- UTF-8 具备**自验证特性**（ 可快速判断是否有效 ）
- GBK **无自描述性**（ 无法仅凭字节流确认正确性 ）
- GB18030 覆盖所有中文编码场景
- 有损 UTF-8 解码确保系统永不崩溃

---

#### 原则 3：有损解码不是失败，而是兜底
很多系统将有损解码视为错误，但在阅读系统中，这种认知并不现实 ( 尽可能的打开所有文件 )。

实际场景中：
- 少量字符丢失 ≪ 系统稳定性受损
- 对于搜索/ embedding：
  - **语义连贯性 > 字节级完美性**

正确的心态：
> **有损解码是兜底方案，而非默认选择**

---

### 1.3 关键细节：GBK 不具备自描述性
GBK / GB2312 的核心问题的：
- 许多字节序列在技术上“有效”
- 但无法可靠验证其解码正确性

这意味着：
- GBK 解码“成功”≠ 结果正确
- 必须用 GB18030 作为更宽泛的安全网

因此，GBK 应谨慎使用，绝不能作为最终解码依据。

### 1.4 编码流水线实现代码
```rust
/// 构建默认的编码探测流水线
pub fn default() -> Self {
    Self {
        decoders: vec![
            Box::new(Utf8Decoder),   // UTF-8 严格验证
            Box::new(GbkDecoder),    // GBK 快速失败
            Box::new(Gb18030Decoder),// GB18030 兼容解码
        ],
    }
}

/// 对输入字节进行编码探测与解码
pub fn decode(&self, input: &[u8]) -> DecodedText {
    // 第一阶段：严格/保守的编码探测
    for d in &self.decoders {
        if let Some(t) = d.decode(input) {
            return t;
        }
    }

    // 第二阶段：兜底抢救（保证永远返回有效结果）
    let (cow, _, _) = UTF_8.decode(input);
    DecodedText {
        text: cow.to_string(),
        encoding: Some("utf-8".into()),
        had_lossy: true,  // 标记有损解码状态
    }
}
```

---

## 二、文本格式：格式 ≠ 内容
### 2.1 格式的本质：远比想象中复杂
| 格式 | 本质是什么 |
|------|------------|
| TXT  | 原始字节流 |
| EPUB | ZIP 压缩包 + XHTML 文档 |
| MOBI | 专有结构 |
| PDF  | 页面渲染指令集 |

它们的唯一共同点：
> **最终都必须转化为纯文本**

---

### 2.2 核心工程决策：v1 版本不追求完整格式保留
> **初期版本的阅读系统，无需保留完整结构格式**

原因如下：
- 对于搜索、嵌入和 AI 阅读场景：
  - **内容本身比排版布局更重要**
- 过早追求结构保留会导致：
  - 解析器复杂度陡增
  - 导入流程与格式强耦合，难以扩展

务实的做法：
> **解码格式 → 提取内容 → 标准化为 UTF-8**

---

### 2.3 推荐架构：分层解耦的流水线
一套干净、可扩展的处理流程：
```
[ 原始字节流 ]
↓
[ 格式解码器 ]   ← 处理 TXT / EPUB / MOBI / PDF 等格式
↓
[ 编码流水线 ]   ← 处理 GBK / GB18030 / UTF-8 等编码
↓
[ UTF-8字节流 ]  ← 统一输出格式
↓
[ 解析/导入/分割/搜索 ]  ← 上层业务逻辑
```

核心优势：
- 格式解码与编码处理**完全正交**（互不依赖）
- Importer 输出：
  - 标准 UTF-8 编码
  - 连续无错乱的字节流
- 上层模块完全无需关心：
  - 原始编码格式
  - 文件类型差异

---

### 2.4 关于 EPUB / PDF 的结构保留：故意推迟的问题
对于格式结构的处理，策略是**分阶段迭代**：
- v1 版本：
  - 优先保证**可读性、可搜索性和系统稳定性**
- v2+ 版本：
  - 在格式解码器层增强结构提取能力
  - 不破坏 UTF-8 字节流的统一契约

换句话说：
> **Importer 接口保持稳定，功能按需迭代升级**

---

## 三、成熟阅读系统的最低标准

- 面对任意编码都不会崩溃
- Importer 之后全链路都是 UTF-8
- 导入和 embedding 流程与格式无关
- 有损解码场景可监控、可追溯

---

## 总结
字符编码和文本格式绝非“微小的实现细节” —— 而是阅读系统的基石。

- 编码标准化越早，系统稳定性越强
- 格式处理越克制，系统扩展性越好